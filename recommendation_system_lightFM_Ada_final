{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing libraries\n",
    "import numpy as np\n",
    "import numpy.ma as ma\n",
    "import pandas as pd\n",
    "# We do this to ignore several specific Pandas warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import sys\n",
    "import scipy.sparse as sparse\n",
    "from scipy.sparse import coo_matrix, csr_matrix\n",
    "from numpy import bincount, log, sqrt\n",
    "import itertools\n",
    "import time\n",
    "import pickle\n",
    "# lightfm \n",
    "from lightfm import LightFM as lightfm\n",
    "from lightfm.cross_validation import random_train_test_split\n",
    "from lightfm.evaluation import auc_score\n",
    "from lightfm.evaluation import precision_at_k\n",
    "from lightfm.evaluation import recall_at_k\n",
    "from lightfm.data import Dataset\n",
    "from sklearn.model_selection import train_test_split as train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note:\n",
    "Throughout the notebook, I applied pickle liberally because 1)many models and evaluations take a long time to run, especially with WARP loss funcions. 2)Pickle allows us to preserve the state of splie between train/test interactions as well as model's state of output for performance comparison.\n",
    "\n",
    "Step 1: Data Preperation\n",
    "In order to prepare the data in a format that works for a recommendation system, we'll need to arrange it in a matrix format where the product id are listed as the the columns, and user ids are listed as the rows.\n",
    "\n",
    "For the \"rating\" part of the equation, although we do not have customers' \"review\" for each product they have purchased, since our expansive dataset covers a customers last 3-99 orders, we can reasonably believe that the number of times a customer has purchase a product throughout their order history is by nature a rating. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading in data. \n",
    "prior = pd.read_csv('order_products__prior.csv')\n",
    "orders = pd.read_csv('orders.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we can see that the prior dataset maps the customer to their order id, but what product is included in each order is hosted in another dataset, orders. We'll merge these two datasets together so that we have a raw dataset that contains both customers and products they have bought across multiple orders. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#displaying the layout of the data sets\n",
    "prior.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#displaying the layout of the data sets\n",
    "orders.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>product_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>14084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>12427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>26088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>26405</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  product_id\n",
       "0        1         196\n",
       "1        1       14084\n",
       "2        1       12427\n",
       "3        1       26088\n",
       "4        1       26405"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#merging prior order with order id\n",
    "customer_orders_product= pd.merge(orders, prior, on=\"order_id\")\n",
    "#extracting only user and product id information \n",
    "customer_orders_product = customer_orders_product[[\"user_id\", \"product_id\"]]\n",
    "customer_orders_product.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataframe customer_orders_product now hosts users and the products they've ordered in the past. Now we move on to count the number of times each user purchased a product as discussed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#summarizing how many of each product each customer bought and creating a new column as \"purchase.\"\n",
    "customer_product_purchase=customer_orders_product.groupby([\"user_id\",\"product_id\"]).size().reset_index(name='purchase')\n",
    "#pickle.dump(customer_product_purchase,open('customer_product_purchase.p','wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the 'customer_product_purchase' dataframe, which now includes three columns: user_id, product_id, purchase, we'll conduct some prelimitary exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#how many unique products are in this dataset?\n",
    "print (\"Unique products in the dataset: \" + str(len(customer_product_purchase['product_id'].unique())))\n",
    "#how many unique customers are there?\n",
    "print (\"Unique customers in the dataset: \" + str(len(customer_product_purchase['user_id'].unique())))\n",
    "#frequency of purchase: what are the most frequently purchase items by customers?\n",
    "customer_product_purchase.nlargest(20, 'purchase')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LightFM requires that the rows and columns of the matrix be the consequtive integers in increasing order; However, our dataset already has user_id in this nature; we'll need to map product_id differently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data can be indexed as the consequtive integers\n",
      "please reformat data\n"
     ]
    }
   ],
   "source": [
    "#checking number of unique users and user_id\n",
    "def index_mapping_check(array):\n",
    "    n = len(array) - 1 \n",
    "    if sum(np.diff(sorted(array)) == 1) >= n:\n",
    "        print (\"data can be indexed as the consequtive integers\")\n",
    "    else:\n",
    "        print (\"please reformat data\")\n",
    "user_id=customer_product_purchase['user_id'].unique().astype('int')\n",
    "product_id=customer_product_purchase['product_id'].unique().astype('int')\n",
    "index_mapping_check(user_id)\n",
    "index_mapping_check(product_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_creation(array):\n",
    "    \"\"\"taking in an array of data and creating an index representing the array.\n",
    "    returning 2 dictionaries: index_id and id_index.\"\"\"\n",
    "    index_id= {}\n",
    "    id_index= {}\n",
    "    for index, id in enumerate(array):\n",
    "        id_index[id] = index\n",
    "        index_id[index] = id\n",
    "    \n",
    "    return index_id,id_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_to_product_id,product_id_to_index=index_creation(product_id)\n",
    "pickle.dump(index_to_product_id, open( \"index_to_product_id.p\", \"wb\" ))\n",
    "pickle.dump(product_id_to_index, open( \"product_id_to_index.p\", \"wb\" ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_id_to_index= pickle.load(open( \"product_id_to_index.p\", \"rb\" ))\n",
    "index_to_product_id=pickle.load(open( \"index_to_product_id.p\", \"rb\" ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#since user_id and product_id are by nature categorical varaibles, we'll encode them as such to prep for the training matrix transformation.\n",
    "customer_product_purchase['user_id']=customer_product_purchase['user_id'].astype('category')\n",
    "customer_product_purchase['product_id']=customer_product_purchase['product_id'].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a train_matrix that has user_id on the rows, product_id as the columns, and purchase as the value.\n",
    "customer_product_purchase_matrix = sparse.coo_matrix((customer_product_purchase['purchase'],(customer_product_purchase['user_id'].cat.codes.copy(),customer_product_purchase['product_id'].apply(lambda x: product_id_to_index[x]).cat.codes.copy())))\n",
    "#saving the meatrix to the file\n",
    "sparse.save_npz('matrix_user_product_purchase.npz', customer_product_purchase_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "#customer_product_purchase_matrix=sparse.load_npz('matrix_user_product_purchase.npz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: Modeling and Evaluation\n",
    "With the prepared matrix, we'll now conduct modeling and evaluation with lightFM's built-in functins. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting train/test matrices with a test percentage of 20% \n",
    "train_matrix, test_matrix=random_train_test_split(customer_product_purchase_matrix,test_percentage=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving the matrix here to preserve the state of the split\n",
    "pickle.dump(train_matrix, open( \"train_matrix.p\", \"wb\" ) )\n",
    "pickle.dump(test_matrix, open( \"test_matrix.p\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a lightfm model instance with all default parameters except the loss function, where the default is logistic\n",
    "model_collaborative_filtering= lightfm(loss = \"warp\")\n",
    "#fitting the model\n",
    "model_collaborative_filtering.fit(train_matrix, epochs=1, num_threads=4)\n",
    "pickle.dump(model_collaborative_filtering, open( \"model_collaborative_filtering.p\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_collaborative_filtering=pickle.load(open( \"model_collaborative_filtering.p\", \"rb\" ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LightFM provides a variety of methods to evaluate the accuracy of the model prediction. To get a \"general idea\" of how well the model fit, we'll first apply AUC score, which randomly takes a pair of postive(confirmed bought by the customer in our case) and negative(confirmed not bought by the customer in our case) and compare their recommendation scores. If the model is accruate, the recommendation score for the positive item should be higher than that of the negative item. A perfect score for AUC is 1, meaning the aforementioned scenario applied to all pairs. Correspondingly, the worst score for AUC is 0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluating the accuracy with auc. Since this part is iterative and time-consuming, we set a timer here to monitor how long it's been running.\n",
    "start = time.time()\n",
    "auc_collaborative_filtering = auc_score(model=model_collaborative_filtering,test_interactions = test_matrix, num_threads = 3, check_intersections = False)\n",
    "end = time.time()\n",
    "\n",
    "print(\"time for evaluation = {0:.{1}f} seconds\".format(end - start, 2))\n",
    "print(\"AUC score = {0:.{1}f}\".format(auc_collaborative_filtering.mean(), 2))\n",
    "pickle.dump(auc_collaborative_filtering, open( \"auc_collaborative_filtering.p\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for precision evaluation, we'll take the average order size from the original prior data here and define k=10. Precison at k measures the percentage of total actually purchased items that ended up amongst top k of the recommendations. A perfect score is 1 and the worst score is 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#precision evaluation\n",
    "start = time.time()\n",
    "Model_precision_collaborative_filtering=precision_at_k(model = model_collaborative_filtering, \n",
    "                        test_interactions = test_matrix,k=10,\n",
    "                        num_threads = 4, check_intersections = False)\n",
    "end = time.time()\n",
    "pickle.dump(Model_precision_collaborative_filtering, open( \"Model_precision_collaborative_filtering.p\", \"wb\" ) )\n",
    "print(\"Precision at k score = {0:.{1}f}\".format(auc_collaborative_filtering.mean(), 2))\n",
    "print(\"time taken for precision at k evaluation = {0:.{1}f} seconds\".format(end - start, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make our recommendation more understandable, we'll now convert the items id back to item names using the product dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_id</th>\n",
       "      <th>product_name</th>\n",
       "      <th>aisle_id</th>\n",
       "      <th>department_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>37116</td>\n",
       "      <td>37117</td>\n",
       "      <td>Health Elderberry Immune Defense Herbal Capsul...</td>\n",
       "      <td>47</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       product_id                                       product_name  \\\n",
       "37116       37117  Health Elderberry Immune Defense Herbal Capsul...   \n",
       "\n",
       "       aisle_id  department_id  \n",
       "37116        47             11  "
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#products=pd.read_csv('products.csv')\n",
    "products[(products.product_id==37117)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_dictionary=products.set_index('product_id')['product_name'].to_dict()\n",
    "#pickle.dump(product_dictionary, open( \"product_dictionary.p\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_recommendation(model, matrix, user_ids):\n",
    "    \"\"\"Takes in a lightFM model, user-item interaction matrix, and list of user ids.\n",
    "    Output the known purchase from a user, and top 3 recommendation based on lightFM \n",
    "    prediction score.\"\"\"\n",
    "\n",
    "    n_users, n_items = matrix.shape\n",
    "    items=range(n_items)\n",
    "    for user_id in user_ids:\n",
    "        known_positives = matrix.tocsr()[user_id].indices\n",
    "        \n",
    "        know_positives_products=[]\n",
    "        for i in known_positives:\n",
    "            know_positives_products.append(product_dictionary[index_to_product_id[i]])\n",
    "\n",
    "        scores = model.predict(user_id, np.arange(n_items))\n",
    "        top_items = np.argsort(-scores)\n",
    "        top_items = top_items[:10]\n",
    "        \n",
    "        top_items_products=[]\n",
    "        for i in top_items:\n",
    "            top_items_products.append(product_dictionary[index_to_product_id[i]])\n",
    "        \n",
    "\n",
    "        print(\"User %s\" % user_id)\n",
    "        print(\"     Customer already have:\")\n",
    "\n",
    "        for x in know_positives_products:\n",
    "            print(\"        %s\" % x)\n",
    "\n",
    "        print(\"     Recommended:\")\n",
    "\n",
    "        for x in top_items_products:\n",
    "            print(\"        %s\" % x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_actual_purchase(matrix, user_ids):\n",
    "    for user_id in user_ids:\n",
    "        actual_purchase = matrix.tocsr()[user_id].indices  \n",
    "        actual_purchase_products=[]\n",
    "        for i in actual_purchase:\n",
    "            actual_purchase_products.append(product_dictionary[index_to_product_id[i]])\n",
    "        print(\"User %s\" % user_id)\n",
    "        print (\"     Customer already have:\")\n",
    "\n",
    "        for x in actual_purchase_products:\n",
    "            print(\"        %s\" % x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_similar_tags(model, product_id):\n",
    "    # Define similarity as the cosine of the angle\n",
    "    # between the tag latent vectors\n",
    "\n",
    "    # Normalize the vectors to unit length\n",
    "    tag_embeddings = (model.item_embeddings.T\n",
    "                      / np.linalg.norm(model.item_embeddings, axis=1)).T\n",
    "\n",
    "    query_embedding = tag_embeddings[product_id_to_index[product_id]]\n",
    "    similarity = np.dot(tag_embeddings, query_embedding)\n",
    "    most_similar = np.argsort(-similarity)[1:10]\n",
    "    most_similar_products=[product_dictionary[index_to_product_id[i]]for i in most_similar]\n",
    "    \n",
    "\n",
    "    return most_similar_products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_embeddings = (model_collaborative_filtering.item_embeddings.T\n",
    "                      / np.linalg.norm(model_collaborative_filtering.item_embeddings, axis=1)).T\n",
    "\n",
    "query_embedding = tag_embeddings[product_id_to_index[37117]]\n",
    "similarity = np.dot(tag_embeddings, query_embedding)\n",
    "most_similar = np.argsort(-similarity)[1:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After testing out the collaborative fitering model, we'll add in the item and user features. In the previous step, we manually created a matrix of user-product interaction. With the new model, we'll try out a different method and use lightFM's built-in dataset tools to create and merge user-product interaction and user/product features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a custome_gender dataframe that classifies customers that bought products from feminine care aisle as female, and others as male.\n",
    "product_feature=products[[\"product_id\",\"aisle_id\"]]\n",
    "customer_product_aisle=pd.merge(product_feature,customer_orders_product, on=\"product_id\")\n",
    "def label_gender (i):\n",
    "    if i == 126:\n",
    "        return 'f'\n",
    "    else:\n",
    "        return 'm'\n",
    "\n",
    "customer_product_aisle['gender']=customer_product_aisle['aisle_id'].apply(lambda x: label_gender(x))\n",
    "customer_gender=customer_product_aisle[['user_id','gender']]\n",
    "customer_gender['gender']=customer_gender['gender'].astype('category')\n",
    "#pickle.dump(customer_gender,open(\"customer_gender.p\",\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#starting a dataset incident and fitting it to the original cusotmer_product_purchase datafrmae\n",
    "dataset = Dataset()\n",
    "dataset.fit((x[0] for x in customer_product_purchase.itertuples(index=False)),\n",
    "            (x[1] for x in customer_product_purchase.itertuples(index=False)))\n",
    "\n",
    "#fitting the same dataframe with user feature\n",
    "dataset.fit_partial(users=(x[0] for x in customer_gender.itertuples(index=False)),items=None,\n",
    "            user_features=(x[1] for x in customer_gender.itertuples(index=False)))\n",
    "\n",
    "#building user-product interaction as interaction. weights is another product from the function that's not as relevent ot our use case.\n",
    "interactions, weights = dataset.build_interactions(((x[0], x[1])\n",
    "                                                      for x in customer_product_purchase.itertuples(index=False)))\n",
    "#build user feature\n",
    "user_features = dataset.build_user_features((x[0], [x[1]])\n",
    "                                              for x in customer_gender.itertuples(index=False))\n",
    "\n",
    "#split the train-test matrices\n",
    "train_interactions, test_interactions=random_train_test_split(interactions,test_percentage=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<lightfm.lightfm.LightFM at 0x10cf96a10>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#creating a lightfm model instance with all default parameters except the loss function, where the default is logistic\n",
    "model_hybrid= lightfm(loss = \"warp\")\n",
    "#fitting the model with additional user features\n",
    "model_hybrid.fit(train_interactions,\n",
    "                 user_features=user_features, \n",
    "          epochs=1, \n",
    "          num_threads=4)\n",
    "pickle.dump(model_hybrid,open('model_hybrid.p','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time for evaluation = 483.87 seconds\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'auc_collaborative_filtering' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-63-56876863e77c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"time for evaluation = {0:.{1}f} seconds\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mend\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"AUC score for hybrid method= {0:.{1}f}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mauc_collaborative_filtering\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'auc_collaborative_filtering' is not defined"
     ]
    }
   ],
   "source": [
    "#evaluating the accuracy with auc. Since this part is iterative and time-consuming, we set a timer here to monitor how long it's been running.\n",
    "start = time.time()\n",
    "auc_hybrid = auc_score(model=model_hybrid,test_interactions = train_interactions, num_threads = 4, check_intersections = False,user_features=user_features,item_features=None)\n",
    "end = time.time()\n",
    "\n",
    "print(\"time for evaluation = {0:.{1}f} seconds\".format(end - start, 2))\n",
    "print(\"AUC score for hybrid method= {0:.{1}f}\".format(auc_hybrid.mean(), 2))\n",
    "#pickle.dump(auc_hybrid,open(\"auc_hybrid.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision score for hybrid method= 0.05\n",
      "time taken for precision at k evaluation = 396.53 seconds\n"
     ]
    }
   ],
   "source": [
    "#precision evaluation\n",
    "start = time.time()\n",
    "model_precision_hybrid=precision_at_k(model = model_hybrid, \n",
    "                        test_interactions = test_interactions,k=10,user_features=user_features, item_features=None,\n",
    "                        num_threads = 4, check_intersections = False)\n",
    "end = time.time()\n",
    "#pickle.dump(model_precision_hybrid, open(\"model_precision_hybrid.p\", \"wb\" ) )\n",
    "print(\"precision score for hybrid method= {0:.{1}f}\".format(model_precision_hybrid.mean(), 2))\n",
    "print(\"time taken for precision at k evaluation = {0:.{1}f} seconds\".format(end - start, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we add on product feature to see if there's any additional improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset2 = Dataset()\n",
    "dataset2.fit((x[0] for x in customer_product_purchase.itertuples(index=False)),(x[1] for x in customer_product_purchase.itertuples(index=False)))\n",
    "\n",
    "dataset2.fit_partial(users=(x[0] for x in customer_gender.itertuples(index=False)),\n",
    "                     items=(x[0] for x in product_feature.itertuples(index=False)),\n",
    "                     user_features=(x[1] for x in customer_gender.itertuples(index=False)),\n",
    "                     item_features=(x[1] for x in product_feature.itertuples(index=False)))\n",
    "\n",
    "interactions2, weights2 = dataset2.build_interactions(((x[0], x[1])\n",
    "                                                      for x in customer_product_purchase.itertuples(index=False)))\n",
    "\n",
    "user_features2 = dataset2.build_user_features((x[0], [x[1]])\n",
    "                                              for x in customer_gender.itertuples(index=False))\n",
    "item_features2 = dataset2.build_item_features((x[0], [x[1]])\n",
    "                                              for x in product_feature.itertuples(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting train/test matrices with a test percentage of 20%\n",
    "train_matrix2, test_matrix2=random_train_test_split(interactions2,test_percentage=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a lightfm model instance with all default parameters except the loss function, where the default is logistic\n",
    "model_hybrid2= lightfm(loss = \"warp\")\n",
    "#fitting the model\n",
    "model_hybrid2.fit(train_matrix2,\n",
    "                  user_features=user_features2,\n",
    "                  item_features=item_features2,\n",
    "                  epochs=1, \n",
    "                  num_threads=4)\n",
    "#pickle.dump(model_hybrid2, open(\"model_hybrid2.p\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time for evaluation = 451.82 seconds\n",
      "AUC score for hybrid method= 0.92\n"
     ]
    }
   ],
   "source": [
    "#evaluating the accuracy with auc. Since this part is iterative and time-consuming, we set a timer here to monitor how long it's been running.\n",
    "start = time.time()\n",
    "auc_hybrid2 = auc_score(model=model_hybrid2,test_interactions = test_matrix2, num_threads = 4, check_intersections = False,user_features=user_features2,item_features=item_features2)\n",
    "end = time.time()\n",
    "#pickle.dump(auc_hybrid2, open(\"auc_hybrid2.p\", \"wb\" ) )\n",
    "print(\"time for evaluation = {0:.{1}f} seconds\".format(end - start, 2))\n",
    "print(\"AUC score for hybrid method= {0:.{1}f}\".format(auc_hybrid2.mean(), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision score for hybrid method= 0.04874\n",
      "time taken for precision at k evaluation = 455.46 seconds\n"
     ]
    }
   ],
   "source": [
    "#precision evaluation\n",
    "start = time.time()\n",
    "model_precision_hybrid2=precision_at_k(model = model_hybrid2, \n",
    "                        test_interactions = test_matrix2,k=10,user_features=user_features2, item_features=item_features2,\n",
    "                        num_threads = 4, check_intersections = False)\n",
    "end = time.time()\n",
    "#pickle.dump(model_precision_hybrid2, open(\"model_precision_hybrid2.p\", \"wb\" ) )\n",
    "print(\"precision score for hybrid method= {0:.{1}f}\".format(model_precision_hybrid2.mean(), 2))\n",
    "print(\"time taken for precision at k evaluation = {0:.{1}f} seconds\".format(end - start, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So using both item and user feature didn't really improve the model performance beyond using user feature alone. What about using item feature by itself?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset3 = Dataset()\n",
    "dataset3.fit((x[0] for x in customer_product_purchase.itertuples(index=False)),(x[1] for x in customer_product_purchase.itertuples(index=False)))\n",
    "\n",
    "dataset3.fit_partial(items=(x[0] for x in product_feature.itertuples(index=False)),\n",
    "                     item_features=(x[1] for x in product_feature.itertuples(index=False)))\n",
    "\n",
    "interactions3, weights3 = dataset3.build_interactions(((x[0], x[1])\n",
    "                                                      for x in customer_product_purchase.itertuples(index=False)))\n",
    "\n",
    "item_features3 = dataset3.build_item_features((x[0], [x[1]])\n",
    "                                              for x in product_feature.itertuples(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_matrix3, test_matrix3=random_train_test_split(interactions3,test_percentage=0.2)\n",
    "#pickle.dump(train_matrix3, open( \"train_matrix3.p\", \"wb\" ) )\n",
    "#pickle.dump(test_matrix3, open( \"test_matrix3.p\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a lightfm model instance with all default parameters except the loss function, where the default is logistic\n",
    "model_hybrid3=lightfm(loss = \"warp\")\n",
    "#fitting the model\n",
    "model_hybrid3.fit(train_matrix3,\n",
    "                  item_features=item_features3,\n",
    "                  epochs=1, \n",
    "                  num_threads=4)\n",
    "pickle.dump(model_hybrid3, open(\"model_hybrid3.p\", \"wb\" ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time for evaluation = 459.28 seconds\n",
      "AUC score for hybrid method= 0.93\n"
     ]
    }
   ],
   "source": [
    "#evaluating the accuracy with auc. Since this part is iterative and time-consuming, we set a timer here to monitor how long it's been running.\n",
    "start = time.time()\n",
    "auc_hybrid3 = auc_score(model=model_hybrid3,test_interactions = test_matrix3, num_threads = 4, check_intersections = False,item_features=item_features3)\n",
    "end = time.time()\n",
    "pickle.dump(auc_hybrid3, open(\"auc_hybrid3.p\", \"wb\" ) )\n",
    "print(\"time for evaluation = {0:.{1}f} seconds\".format(end - start, 2))\n",
    "print(\"AUC score for hybrid method= {0:.{1}f}\".format(auc_hybrid3.mean(), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision score for hybrid method= 0.05\n",
      "time taken for precision at k evaluation = 453.17 seconds\n"
     ]
    }
   ],
   "source": [
    "#precision evaluation\n",
    "start = time.time()\n",
    "model_precision_hybrid3=precision_at_k(model = model_hybrid3, \n",
    "                        test_interactions = test_matrix3,k=10,item_features=item_features3,\n",
    "                        num_threads = 4, check_intersections = False)\n",
    "end = time.time()\n",
    "pickle.dump(model_precision_hybrid3, open(\"model_precision_hybrid3.p\", \"wb\" ) )\n",
    "print(\"precision score for hybrid method= {0:.{1}f}\".format(model_precision_hybrid3.mean(), 2))\n",
    "print(\"time taken for precision at k evaluation = {0:.{1}f} seconds\".format(end - start, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply to Train\n",
    "Note: This part of the code was not presented.\n",
    "\n",
    "In the previous section, we applied the lightFM model on the \"prior\" dataset. The data in the prior dataset is aggregation in natare, and the grain of the dataset is user.\n",
    "In the \"train\" dataset, we have one order from each individual user, which provides a unique opporunity for us to test how lightFM would perform on an order basis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading into the dataset\n",
    "train=pd.read_csv('order_products__train.csv')\n",
    "#merging prior order with order id\n",
    "customer_orders_product_train= pd.merge(orders, train, on=\"order_id\")\n",
    "#extracting only user and product id information \n",
    "customer_orders_product_train = customer_orders_product_train[[\"user_id\", \"product_id\"]]\n",
    "customer_product_purchase_train=customer_orders_product_train.groupby([\"user_id\",\"product_id\"]).size().reset_index(name='purchase')\n",
    "customer_product_purchase_train[\"user_id\"]=customer_orders_product_train[\"user_id\"].astype('category')\n",
    "customer_product_purchase_train[\"product_id\"]=customer_orders_product_train[\"product_id\"].astype('category')\n",
    "pickle.dump(customer_product_purchase_train, open(\"customer_product_purchase_train.p\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a dataset using the lightFM native methods\n",
    "dataset_train = Dataset()\n",
    "dataset_train.fit((x[0] for x in customer_product_purchase_train.itertuples(index=False)),(x[1] for x in customer_product_purchase_train.itertuples(index=False)))\n",
    "\n",
    "dataset_train.fit_partial(users=(x[0] for x in customer_gender.itertuples(index=False)),\n",
    "                     items=(x[0] for x in product_feature.itertuples(index=False)),\n",
    "                     user_features=(x[1] for x in customer_gender.itertuples(index=False)),\n",
    "                     item_features=(x[1] for x in product_feature.itertuples(index=False)))\n",
    "\n",
    "user_features_train = dataset_train.build_user_features((x[0], [x[1]])\n",
    "                                              for x in customer_gender.itertuples(index=False))\n",
    "item_features_train = dataset_train.build_user_features((x[0], [x[1]])\n",
    "                                              for x in product_feature.itertuples(index=False))\n",
    "\n",
    "interactions_train, weights_train = dataset_train.build_interactions(((x[0], x[1])\n",
    "                                                      for x in customer_product_purchase_train.itertuples(index=False)))\n",
    "\n",
    "train_matrix_modeling, test_matrix_testing=random_train_test_split(interactions_train,test_percentage=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a lightfm model instance with all default parameters except the loss function, where the default is logistic\n",
    "model_hybrid_train= lightfm(loss = \"warp\")\n",
    "#fitting the model with user features and item features on the 'train' dataset\n",
    "model_hybrid_train.fit(train_matrix_modeling,\n",
    "                 user_features=user_features_train,\n",
    "                 item_features=item_features_train,\n",
    "                       epochs=1, \n",
    "                       num_threads=4)\n",
    "pickle.dump(model_hybrid_train, open(\"model_hybrid_train.p\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time for evaluation = 192.90 seconds\n",
      "AUC score for hybrid method= 0.92\n"
     ]
    }
   ],
   "source": [
    "#evaluating the accuracy with auc. Since this part is iterative and time-consuming, we set a timer here to monitor how long it's been running.\n",
    "start = time.time()\n",
    "auc_hybrid_train = auc_score(model=model_hybrid_train,test_interactions = test_matrix_testing, num_threads = 4, check_intersections = False,user_features=user_features_train,item_features=item_features_train)\n",
    "end = time.time()\n",
    "\n",
    "print(\"time for evaluation = {0:.{1}f} seconds\".format(end - start, 2))\n",
    "print(\"AUC score for hybrid method= {0:.{1}f}\".format(auc_hybrid_train.mean(), 2))\n",
    "pickle.dump(auc_hybrid_train, open(\"auc_hybrid_train.p\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sad moment when we realized that applying user and item feature to the 'train' dataset gets us an abismal precision at k score..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision score for hybrid method= 0.01\n",
      "time taken for precision at k evaluation = 197.40 seconds\n"
     ]
    }
   ],
   "source": [
    "#precision evaluation\n",
    "start = time.time()\n",
    "model_precision_hybrid_train=precision_at_k(model = model_hybrid_train, \n",
    "                        test_interactions = test_matrix_testing,k=10,user_features=user_features_train, item_features=item_features_train,\n",
    "                        num_threads = 4, check_intersections = False)\n",
    "end = time.time()\n",
    "pickle.dump(model_precision_hybrid_train, open(\"model_precision_hybrid_train.p\", \"wb\" ) )\n",
    "print(\"precision score for hybrid method= {0:.{1}f}\".format(model_precision_hybrid_train.mean(), 2))\n",
    "print(\"time taken for precision at k evaluation = {0:.{1}f} seconds\".format(end - start, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the results above...maybe we are creating too much noise with additional features since a lot of items have missing features. let's try it again with only user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train_noitem = Dataset()\n",
    "dataset_train_noitem.fit((x[0] for x in customer_product_purchase_train.itertuples(index=False)),(x[1] for x in customer_product_purchase_train.itertuples(index=False)))\n",
    "\n",
    "dataset_train_noitem.fit_partial(users=(x[0] for x in customer_gender.itertuples(index=False)),\n",
    "                     user_features=(x[1] for x in customer_gender.itertuples(index=False)))\n",
    "\n",
    "user_features_train_noitem = dataset_train.build_user_features((x[0], [x[1]])\n",
    "                                              for x in customer_gender.itertuples(index=False))\n",
    "\n",
    "interactions_train_noitem, weights_train_noitem = dataset_train.build_interactions(((x[0], x[1])\n",
    "                                                      for x in customer_product_purchase_train.itertuples(index=False)))\n",
    "\n",
    "train_matrix_noitem, test_matrix_noitem=random_train_test_split(interactions_train_noitem,test_percentage=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a lightfm model instance with all default parameters except the loss function, where the default is logistic\n",
    "model_hybrid_train_noitem= lightfm(loss = \"warp\")\n",
    "#fitting the model\n",
    "model_hybrid_train_noitem.fit(train_matrix_noitem,\n",
    "                 user_features=user_features_train_noitem,\n",
    "                 item_features=None,\n",
    "          epochs=1, \n",
    "          num_threads=4)\n",
    "#pickle.dump(model_hybrid_train_noitem, open(\"model_hybrid_train_noitem.p\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time for evaluation = 180.77 seconds\n",
      "AUC score for hybrid method= 0.92\n"
     ]
    }
   ],
   "source": [
    "#evaluating the accuracy with auc. Since this part is iterative and time-consuming, we set a timer here to monitor how long it's been running.\n",
    "start = time.time()\n",
    "auc_hybrid_train_noitem= auc_score(model=model_hybrid_train_noitem,test_interactions = test_matrix_noitem, num_threads = 4, check_intersections = False,user_features=user_features_train_noitem,item_features=None)\n",
    "end = time.time()\n",
    "#pickle.dump(auc_hybrid_train_noitem, open(\"auc_hybrid_train_noitem.p\", \"wb\" ) )\n",
    "print(\"time for evaluation = {0:.{1}f} seconds\".format(end - start, 2))\n",
    "print(\"AUC score for hybrid method= {0:.{1}f}\".format(auc_hybrid_train_noitem.mean(), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision score for hybrid method= 0.02\n",
      "time taken for precision at k evaluation = 181.45 seconds\n"
     ]
    }
   ],
   "source": [
    "#precision evaluation\n",
    "start = time.time()\n",
    "model_precision_hybrid_train_noitem=precision_at_k(model = model_hybrid_train_noitem, \n",
    "                        test_interactions = test_matrix_noitem,k=10,user_features=user_features_train_noitem, item_features=None,\n",
    "                        num_threads = 4, check_intersections = False)\n",
    "end = time.time()\n",
    "#pickle.dump(model_precision_hybrid_train_noitem, open(\"model_precision_hybrid_train_noitem.p\", \"wb\" ) )\n",
    "print(\"precision score for hybrid method= {0:.{1}f}\".format(model_precision_hybrid_train_noitem.mean(), 2))\n",
    "print(\"time taken for precision at k evaluation = {0:.{1}f} seconds\".format(end - start, 2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
